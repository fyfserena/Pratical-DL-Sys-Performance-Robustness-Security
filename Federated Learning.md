Paper: Advances and Open Problems in Federated Learning https://arxiv.org/abs/1912.04977

Defending Against Attacks and Failures

Modern machine learning systems can be vulnerable to various kinds of failures:
* nonmalicious failures such as bugs in preprocessing pipelines, noisy training labels, unreliable clients
* explicit attacks that target training and deployment pipelines

1. Adversarial Attacks on Model Performance

* Examples of adversarial attacks include 
  data poisoning (training-time attacks):  the adversary alters the client datasets used to train the model
  model update poisoning: the adversary alters model updates sent to the server
  model evasion attacks (inference-time attacks): the adversary alters the data used at inference-time
  
* Federated learning may introduce new attack surfaces at training-time
* New attack vectors on federated training systems may be combined with novel adversarial inference-time attacks

- untargeted attacks = model downgrade attacks => reduce the model’s global accuracy, or “fully break” the global model
- targeted attacks = backdoor attacks => alter the model’s behavior on a minority of examples while maintaining good overall accuracy on all other examples

Byzantine attacks:
* untargeted model update poisoning attacks
* Byzantine clients can send arbitrary values to the server

Byzantine-resilient defenses:
* replaces the averaging step on the server with a robust estimate of the mean, 
* such as median-based aggregators, Krum, and trimmed mean
* more empirical analyses of the effectiveness of Byzantine-resilient defenses in federated learning may be necessary
* since the theoretical guarantees of these defenses may only hold under assumptions on the learning problem that are often not met

backdoor:
* hard to detect
* These attacks have been extended to federated meta-learning, where backdoors inserted via one-shot attacks are shown to persist for tens of training rounds.
* An interesting avenue for future work would be to explore the use of zero-knowledge proofs to ensure that users are submitting updates with pre-specified properties.

Data poisoning:
* only manipulate client data, perhaps by replacing labels or specific features of the data
* [good] Byzantine-robust aggregators successfully defended against label-flipping data poisoning attacks on convolutional neural networks
* [bad] Non-IID data and unreliability of clients all present serious challenges and disrupt common assumptions in works on Byzantine-robust aggregation.
* Defense mechnism: data sanitization and network pruning
  - remove poisoned otherwise anomalous data
  - use robust statistics
  - [cool] remove activation units that are inactive on clean data
* Neither data sanitization nor network pruning work directly in federated settings, as they both generallyrequire access to client data

Evasion attacks:
* circumvent a deployed model by carefully manipulating samples that are fed into the model (“adversarial examples”)
* adversarial examples are generally constructed by adding norm-bounded perturbations to test examples
  - white-box: perturbations are generated by attempting to maximize the loss function subject to a norm constraint via constrained optimization methods such as projected gradient ascent
  - black-box : attacks based on query-access to the model or based on substitute models trained on similar data
* setting appropriate bounds on the norm of perturbations to perform adversarial training (a challenging problem even in the IID setting [453]) becomes harder
in federated settings where the training data cannot be inspected ahead of training
* generating adversarial examples is relatively expensive
* new on-device robust optimization techniques may be required in the federated learning setting

Defending against model update poisoning attacks: 
The service provider can bound the contribution of any individual client to the overall model by 
(1) enforcing a norm constraint on the client model update (e.g. by clipping the client updates), 
(2) aggregating the clipped updates, 
(3) and adding Gaussian noise to the aggregate

Defending against data poisoning attacks:
Key: improving robustness => differential privacy as a defense against data poisoning <= noise must be injected into the learning procedure
