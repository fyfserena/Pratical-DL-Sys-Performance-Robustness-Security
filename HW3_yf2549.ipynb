{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_yf2549.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiOQesSWd0Zug7GobwDWoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fyfserena/Pratical-DL-Sys-Performance-Robustness-Security/blob/main/HW3_yf2549.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_2UdXEohKiY"
      },
      "source": [
        "## Yingfei Fan (yf2549) COMS E6998 section 012 HW3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyPAIH-XhIXe"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "import time \n",
        "\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJBepgxOiJDS"
      },
      "source": [
        "## Problem 1 - Adaptive Learning Rate Methods, CIFAR-10\n",
        "\n",
        "We will consider five methods, AdaGrad, RMSProp, RMSProp+Nesterov, AdaDelta, Adam, and study their convergence using CIFAR-10 dataset. \n",
        "We will use multi-layer neural network model with two fully connected hidden layers with 1000 hidden units each and ReLU activation with minibatch size of 128.\n",
        "\n",
        "1. Write the weight update equations for the five adaptive learning rate methods. Explain each term\n",
        "clearly. What are the hyperparameters in each policy ? Explain how AdaDelta and Adam are different\n",
        "from RMSProp. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbUKny-0icu8"
      },
      "source": [
        "**Solution:**\n",
        "\n",
        "1.AdaGrad: \n",
        "$$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{diag(G_{t}) + \\epsilon I}}  g_{t}$$\n",
        "\n",
        "* $\\theta$: parameter we are training\n",
        "* $g_t$ is the gradient at time step $t$\n",
        "* diag($G_t$) is a diagonal matrix where each diagonal element $i$, $i$ is the sum of squares of the gradients with respect to $\\theta$ up to time step $t$.\n",
        "* Hyperparameters:\n",
        "  -  $\\eta$: the learning rate\n",
        "  -  $\\epsilon$ is a smoothing term that prevents division by $0$.\n",
        "* Adagrad uses a different learning rate for every parameter $\\theta_i$ at every time step $t$.\n",
        "\n",
        "2.RMSProp: \n",
        "$$\\begin{align} \n",
        "\\begin{split} \n",
        "E[g^2]_t &= 0.9 E[g^2]_{t-1} + 0.1 g^2_t \\\\ \n",
        "\\theta_{t+1} &= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} g_{t} \n",
        "\\end{split} \n",
        "\\end{align}$$\n",
        "\n",
        "* The terms here share the same meaning metioned above.\n",
        "\n",
        "3.RMSProp+Nesterov: $$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} (\\beta_1 \\hat{m}_t + \\dfrac{(1 - \\beta_1) g_t}{1 - \\beta^t_1})$$\n",
        "\n",
        "* $\\gamma$ is the momentum decay term. $\\beta_1$ is the decay rate.\n",
        "\n",
        "4.AdaDelta: \n",
        "$$\\begin{align} \n",
        "\\begin{split} \n",
        "\\Delta \\theta_t &= - \\dfrac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_{t}} g_{t} \\\\ \n",
        "\\theta_{t+1} &= \\theta_t + \\Delta \\theta_t \n",
        "\\end{split} \n",
        "\\end{align}$$\n",
        "\n",
        "* RMS is the root mean squared errors. \n",
        "* Instead of inefficiently storing w previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients.\n",
        "\n",
        "5.Adam: \n",
        "$$\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
        "\n",
        "* $m_t$: the estimate of the first moment of the gradients.\n",
        "* $v_t$: the estimate of the second moment of the gradients.\n",
        "\n",
        "\n",
        "\n",
        "6. How AdaDelta and Adam are different from RMSProp?\n",
        "\n",
        "* Adagrad is an algorithm for gradient-based optimization that does just this:  \n",
        "  * It adapts the learning rate to the parameters, performing smaller updates\n",
        "  (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. \n",
        "  * For this reason, it is well-suited for dealing with sparse data.\n",
        "  * Adagrad's main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.\n",
        "\n",
        "* RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates.\n",
        "\n",
        "* Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size $w$.\n",
        "\n",
        "* RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. \n",
        "\n",
        "* Adam:\n",
        "In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum. \n",
        "\n",
        "Reference: https://ruder.io/optimizing-gradient-descent/index.html#adagrad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-MqPhSGiTgq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kOgxUFqtBEV"
      },
      "source": [
        "2. Train the neural network using all the five methods with L2-regularization for 200 epochs each and plot\n",
        "the training loss vs number of epochs. Which method performs best (lowest training loss) ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVPYiJVEtBed",
        "outputId": "80fbc593-0247-4a8e-c276-612a555bcb76"
      },
      "source": [
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255.0\n",
        "X_test /= 255.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnM1t-c1tW_A",
        "outputId": "4a69eebe-fe80-44b1-93c8-9b71d2b51f28"
      },
      "source": [
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 32, 32, 3), (50000, 10), (10000, 32, 32, 3), (10000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf-Efsi6uCJs"
      },
      "source": [
        "#Model\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1000, activation='relu', kernel_regularizer='l2', kernel_initializer='HeNormal'))\n",
        "model.add(Dense(1000, activation='relu', kernel_regularizer='l2', kernel_initializer='HeNormal'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgB41meducRM",
        "outputId": "cc92ec0d-9e9f-4ec5-96b5-22cbb062fcd4"
      },
      "source": [
        "#Adagrad\n",
        "tf.keras.optimizers.Adagrad(learning_rate=0.01, initial_accumulator_value=0.1, epsilon=1e-07, name='Adagrad')\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_adagrad = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "782/782 - 49s - loss: 6.2790 - accuracy: 0.4649 - val_loss: 6.0938 - val_accuracy: 0.4439\n",
            "Epoch 2/200\n",
            "782/782 - 48s - loss: 5.8510 - accuracy: 0.4699 - val_loss: 5.7413 - val_accuracy: 0.4189\n",
            "Epoch 3/200\n",
            "782/782 - 48s - loss: 5.4702 - accuracy: 0.4710 - val_loss: 5.3224 - val_accuracy: 0.4550\n",
            "Epoch 4/200\n",
            "782/782 - 49s - loss: 5.1280 - accuracy: 0.4728 - val_loss: 5.0068 - val_accuracy: 0.4447\n",
            "Epoch 5/200\n",
            "782/782 - 49s - loss: 4.8225 - accuracy: 0.4755 - val_loss: 4.7096 - val_accuracy: 0.4535\n",
            "Epoch 6/200\n",
            "782/782 - 49s - loss: 4.5456 - accuracy: 0.4755 - val_loss: 4.4419 - val_accuracy: 0.4572\n",
            "Epoch 7/200\n",
            "782/782 - 49s - loss: 4.2958 - accuracy: 0.4784 - val_loss: 4.2108 - val_accuracy: 0.4586\n",
            "Epoch 8/200\n",
            "782/782 - 48s - loss: 4.0705 - accuracy: 0.4779 - val_loss: 4.0272 - val_accuracy: 0.4447\n",
            "Epoch 9/200\n",
            "782/782 - 49s - loss: 3.8670 - accuracy: 0.4805 - val_loss: 3.8010 - val_accuracy: 0.4622\n",
            "Epoch 10/200\n",
            "782/782 - 48s - loss: 3.6819 - accuracy: 0.4816 - val_loss: 3.6211 - val_accuracy: 0.4674\n",
            "Epoch 11/200\n",
            "782/782 - 48s - loss: 3.5149 - accuracy: 0.4821 - val_loss: 3.4741 - val_accuracy: 0.4621\n",
            "Epoch 12/200\n",
            "782/782 - 48s - loss: 3.3630 - accuracy: 0.4825 - val_loss: 3.3135 - val_accuracy: 0.4661\n",
            "Epoch 13/200\n",
            "782/782 - 48s - loss: 3.2240 - accuracy: 0.4837 - val_loss: 3.1789 - val_accuracy: 0.4688\n",
            "Epoch 14/200\n",
            "782/782 - 48s - loss: 3.0970 - accuracy: 0.4854 - val_loss: 3.0697 - val_accuracy: 0.4655\n",
            "Epoch 15/200\n",
            "782/782 - 48s - loss: 2.9830 - accuracy: 0.4877 - val_loss: 2.9548 - val_accuracy: 0.4747\n",
            "Epoch 16/200\n",
            "782/782 - 48s - loss: 2.8776 - accuracy: 0.4888 - val_loss: 2.8599 - val_accuracy: 0.4670\n",
            "Epoch 17/200\n",
            "782/782 - 49s - loss: 2.7826 - accuracy: 0.4873 - val_loss: 2.7597 - val_accuracy: 0.4790\n",
            "Epoch 18/200\n",
            "782/782 - 49s - loss: 2.6946 - accuracy: 0.4906 - val_loss: 2.6809 - val_accuracy: 0.4727\n",
            "Epoch 19/200\n",
            "782/782 - 49s - loss: 2.6141 - accuracy: 0.4921 - val_loss: 2.6098 - val_accuracy: 0.4744\n",
            "Epoch 20/200\n",
            "782/782 - 48s - loss: 2.5411 - accuracy: 0.4910 - val_loss: 2.5313 - val_accuracy: 0.4824\n",
            "Epoch 21/200\n",
            "782/782 - 49s - loss: 2.4727 - accuracy: 0.4930 - val_loss: 2.4825 - val_accuracy: 0.4696\n",
            "Epoch 22/200\n",
            "782/782 - 49s - loss: 2.4113 - accuracy: 0.4931 - val_loss: 2.4329 - val_accuracy: 0.4598\n",
            "Epoch 23/200\n",
            "782/782 - 49s - loss: 2.3542 - accuracy: 0.4959 - val_loss: 2.3535 - val_accuracy: 0.4827\n",
            "Epoch 24/200\n",
            "782/782 - 48s - loss: 2.3020 - accuracy: 0.4965 - val_loss: 2.3124 - val_accuracy: 0.4784\n",
            "Epoch 25/200\n",
            "782/782 - 49s - loss: 2.2540 - accuracy: 0.4977 - val_loss: 2.2812 - val_accuracy: 0.4679\n",
            "Epoch 26/200\n",
            "782/782 - 49s - loss: 2.2098 - accuracy: 0.4976 - val_loss: 2.2274 - val_accuracy: 0.4802\n",
            "Epoch 27/200\n",
            "782/782 - 49s - loss: 2.1684 - accuracy: 0.5000 - val_loss: 2.1807 - val_accuracy: 0.4816\n",
            "Epoch 28/200\n",
            "782/782 - 49s - loss: 2.1306 - accuracy: 0.5002 - val_loss: 2.1490 - val_accuracy: 0.4837\n",
            "Epoch 29/200\n",
            "782/782 - 49s - loss: 2.0956 - accuracy: 0.5014 - val_loss: 2.1210 - val_accuracy: 0.4819\n",
            "Epoch 30/200\n",
            "782/782 - 49s - loss: 2.0632 - accuracy: 0.5020 - val_loss: 2.0898 - val_accuracy: 0.4826\n",
            "Epoch 31/200\n",
            "782/782 - 49s - loss: 2.0329 - accuracy: 0.5019 - val_loss: 2.0470 - val_accuracy: 0.4884\n",
            "Epoch 32/200\n",
            "782/782 - 49s - loss: 2.0049 - accuracy: 0.5027 - val_loss: 2.0273 - val_accuracy: 0.4860\n",
            "Epoch 33/200\n",
            "782/782 - 49s - loss: 1.9787 - accuracy: 0.5049 - val_loss: 2.0343 - val_accuracy: 0.4757\n",
            "Epoch 34/200\n",
            "782/782 - 49s - loss: 1.9558 - accuracy: 0.5039 - val_loss: 1.9734 - val_accuracy: 0.4875\n",
            "Epoch 35/200\n",
            "782/782 - 49s - loss: 1.9336 - accuracy: 0.5058 - val_loss: 1.9582 - val_accuracy: 0.4868\n",
            "Epoch 36/200\n",
            "782/782 - 49s - loss: 1.9127 - accuracy: 0.5064 - val_loss: 1.9487 - val_accuracy: 0.4881\n",
            "Epoch 37/200\n",
            "782/782 - 49s - loss: 1.8931 - accuracy: 0.5075 - val_loss: 1.9331 - val_accuracy: 0.4828\n",
            "Epoch 38/200\n",
            "782/782 - 49s - loss: 1.8758 - accuracy: 0.5073 - val_loss: 1.8989 - val_accuracy: 0.4972\n",
            "Epoch 39/200\n",
            "782/782 - 49s - loss: 1.8588 - accuracy: 0.5089 - val_loss: 1.8815 - val_accuracy: 0.4927\n",
            "Epoch 40/200\n",
            "782/782 - 49s - loss: 1.8433 - accuracy: 0.5080 - val_loss: 1.8714 - val_accuracy: 0.4898\n",
            "Epoch 41/200\n",
            "782/782 - 49s - loss: 1.8281 - accuracy: 0.5104 - val_loss: 1.8571 - val_accuracy: 0.4918\n",
            "Epoch 42/200\n",
            "782/782 - 49s - loss: 1.8146 - accuracy: 0.5121 - val_loss: 1.8491 - val_accuracy: 0.4934\n",
            "Epoch 43/200\n",
            "782/782 - 50s - loss: 1.8016 - accuracy: 0.5112 - val_loss: 1.8577 - val_accuracy: 0.4818\n",
            "Epoch 44/200\n",
            "782/782 - 49s - loss: 1.7894 - accuracy: 0.5109 - val_loss: 1.8221 - val_accuracy: 0.4953\n",
            "Epoch 45/200\n",
            "782/782 - 49s - loss: 1.7777 - accuracy: 0.5135 - val_loss: 1.8085 - val_accuracy: 0.4933\n",
            "Epoch 46/200\n",
            "782/782 - 49s - loss: 1.7669 - accuracy: 0.5122 - val_loss: 1.8064 - val_accuracy: 0.4912\n",
            "Epoch 47/200\n",
            "782/782 - 50s - loss: 1.7567 - accuracy: 0.5155 - val_loss: 1.8095 - val_accuracy: 0.4885\n",
            "Epoch 48/200\n",
            "782/782 - 50s - loss: 1.7480 - accuracy: 0.5150 - val_loss: 1.7836 - val_accuracy: 0.4944\n",
            "Epoch 49/200\n",
            "782/782 - 50s - loss: 1.7390 - accuracy: 0.5155 - val_loss: 1.7957 - val_accuracy: 0.4900\n",
            "Epoch 50/200\n",
            "782/782 - 50s - loss: 1.7300 - accuracy: 0.5167 - val_loss: 1.7631 - val_accuracy: 0.5038\n",
            "Epoch 51/200\n",
            "782/782 - 50s - loss: 1.7225 - accuracy: 0.5177 - val_loss: 1.7664 - val_accuracy: 0.4916\n",
            "Epoch 52/200\n",
            "782/782 - 51s - loss: 1.7153 - accuracy: 0.5167 - val_loss: 1.7690 - val_accuracy: 0.4834\n",
            "Epoch 53/200\n",
            "782/782 - 49s - loss: 1.7083 - accuracy: 0.5186 - val_loss: 1.7517 - val_accuracy: 0.4924\n",
            "Epoch 54/200\n",
            "782/782 - 50s - loss: 1.7007 - accuracy: 0.5182 - val_loss: 1.7452 - val_accuracy: 0.4963\n",
            "Epoch 55/200\n",
            "782/782 - 49s - loss: 1.6945 - accuracy: 0.5204 - val_loss: 1.7663 - val_accuracy: 0.4750\n",
            "Epoch 56/200\n",
            "782/782 - 49s - loss: 1.6878 - accuracy: 0.5218 - val_loss: 1.7286 - val_accuracy: 0.4944\n",
            "Epoch 57/200\n",
            "782/782 - 50s - loss: 1.6824 - accuracy: 0.5223 - val_loss: 1.7646 - val_accuracy: 0.4819\n",
            "Epoch 58/200\n",
            "782/782 - 49s - loss: 1.6774 - accuracy: 0.5209 - val_loss: 1.7226 - val_accuracy: 0.4971\n",
            "Epoch 59/200\n",
            "782/782 - 50s - loss: 1.6717 - accuracy: 0.5230 - val_loss: 1.7242 - val_accuracy: 0.4972\n",
            "Epoch 60/200\n",
            "782/782 - 49s - loss: 1.6672 - accuracy: 0.5216 - val_loss: 1.7169 - val_accuracy: 0.4913\n",
            "Epoch 61/200\n",
            "782/782 - 49s - loss: 1.6619 - accuracy: 0.5216 - val_loss: 1.7176 - val_accuracy: 0.4957\n",
            "Epoch 62/200\n",
            "782/782 - 49s - loss: 1.6582 - accuracy: 0.5216 - val_loss: 1.7065 - val_accuracy: 0.5040\n",
            "Epoch 63/200\n",
            "782/782 - 48s - loss: 1.6534 - accuracy: 0.5244 - val_loss: 1.6974 - val_accuracy: 0.5059\n",
            "Epoch 64/200\n",
            "782/782 - 48s - loss: 1.6495 - accuracy: 0.5242 - val_loss: 1.6969 - val_accuracy: 0.5052\n",
            "Epoch 65/200\n",
            "782/782 - 48s - loss: 1.6448 - accuracy: 0.5250 - val_loss: 1.6907 - val_accuracy: 0.5052\n",
            "Epoch 66/200\n",
            "782/782 - 48s - loss: 1.6417 - accuracy: 0.5258 - val_loss: 1.6841 - val_accuracy: 0.5026\n",
            "Epoch 67/200\n",
            "782/782 - 48s - loss: 1.6374 - accuracy: 0.5260 - val_loss: 1.6878 - val_accuracy: 0.5064\n",
            "Epoch 68/200\n",
            "782/782 - 48s - loss: 1.6337 - accuracy: 0.5264 - val_loss: 1.7199 - val_accuracy: 0.4893\n",
            "Epoch 69/200\n",
            "782/782 - 48s - loss: 1.6305 - accuracy: 0.5261 - val_loss: 1.6815 - val_accuracy: 0.5067\n",
            "Epoch 70/200\n",
            "782/782 - 48s - loss: 1.6271 - accuracy: 0.5257 - val_loss: 1.6889 - val_accuracy: 0.4948\n",
            "Epoch 71/200\n",
            "782/782 - 48s - loss: 1.6241 - accuracy: 0.5278 - val_loss: 1.7375 - val_accuracy: 0.4742\n",
            "Epoch 72/200\n",
            "782/782 - 48s - loss: 1.6209 - accuracy: 0.5280 - val_loss: 1.6869 - val_accuracy: 0.4915\n",
            "Epoch 73/200\n",
            "782/782 - 48s - loss: 1.6186 - accuracy: 0.5286 - val_loss: 1.6819 - val_accuracy: 0.4950\n",
            "Epoch 74/200\n",
            "782/782 - 48s - loss: 1.6155 - accuracy: 0.5291 - val_loss: 1.6845 - val_accuracy: 0.4995\n",
            "Epoch 75/200\n",
            "782/782 - 48s - loss: 1.6124 - accuracy: 0.5306 - val_loss: 1.6587 - val_accuracy: 0.5107\n",
            "Epoch 76/200\n",
            "782/782 - 48s - loss: 1.6098 - accuracy: 0.5298 - val_loss: 1.6605 - val_accuracy: 0.5046\n",
            "Epoch 77/200\n",
            "782/782 - 48s - loss: 1.6071 - accuracy: 0.5311 - val_loss: 1.6575 - val_accuracy: 0.5059\n",
            "Epoch 78/200\n",
            "782/782 - 48s - loss: 1.6052 - accuracy: 0.5295 - val_loss: 1.6644 - val_accuracy: 0.5076\n",
            "Epoch 79/200\n",
            "782/782 - 48s - loss: 1.6026 - accuracy: 0.5308 - val_loss: 1.6624 - val_accuracy: 0.5054\n",
            "Epoch 80/200\n",
            "782/782 - 48s - loss: 1.5998 - accuracy: 0.5323 - val_loss: 1.6521 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "782/782 - 48s - loss: 1.5982 - accuracy: 0.5320 - val_loss: 1.6487 - val_accuracy: 0.5069\n",
            "Epoch 82/200\n",
            "782/782 - 48s - loss: 1.5956 - accuracy: 0.5329 - val_loss: 1.6545 - val_accuracy: 0.5085\n",
            "Epoch 83/200\n",
            "782/782 - 48s - loss: 1.5935 - accuracy: 0.5330 - val_loss: 1.6831 - val_accuracy: 0.4883\n",
            "Epoch 84/200\n",
            "782/782 - 48s - loss: 1.5922 - accuracy: 0.5324 - val_loss: 1.6571 - val_accuracy: 0.5055\n",
            "Epoch 85/200\n",
            "782/782 - 49s - loss: 1.5895 - accuracy: 0.5356 - val_loss: 1.6460 - val_accuracy: 0.5077\n",
            "Epoch 86/200\n",
            "782/782 - 49s - loss: 1.5874 - accuracy: 0.5336 - val_loss: 1.6526 - val_accuracy: 0.5076\n",
            "Epoch 87/200\n",
            "782/782 - 49s - loss: 1.5853 - accuracy: 0.5339 - val_loss: 1.6657 - val_accuracy: 0.5003\n",
            "Epoch 88/200\n",
            "782/782 - 48s - loss: 1.5833 - accuracy: 0.5357 - val_loss: 1.6391 - val_accuracy: 0.5033\n",
            "Epoch 89/200\n",
            "782/782 - 48s - loss: 1.5814 - accuracy: 0.5361 - val_loss: 1.6454 - val_accuracy: 0.5038\n",
            "Epoch 90/200\n",
            "782/782 - 49s - loss: 1.5809 - accuracy: 0.5345 - val_loss: 1.6394 - val_accuracy: 0.5125\n",
            "Epoch 91/200\n",
            "782/782 - 49s - loss: 1.5787 - accuracy: 0.5360 - val_loss: 1.7034 - val_accuracy: 0.4792\n",
            "Epoch 92/200\n",
            "782/782 - 49s - loss: 1.5770 - accuracy: 0.5368 - val_loss: 1.6388 - val_accuracy: 0.5009\n",
            "Epoch 93/200\n",
            "782/782 - 49s - loss: 1.5749 - accuracy: 0.5390 - val_loss: 1.6339 - val_accuracy: 0.5138\n",
            "Epoch 94/200\n",
            "782/782 - 49s - loss: 1.5736 - accuracy: 0.5371 - val_loss: 1.6385 - val_accuracy: 0.5013\n",
            "Epoch 95/200\n",
            "782/782 - 49s - loss: 1.5722 - accuracy: 0.5372 - val_loss: 1.6254 - val_accuracy: 0.5189\n",
            "Epoch 96/200\n",
            "782/782 - 49s - loss: 1.5703 - accuracy: 0.5381 - val_loss: 1.6352 - val_accuracy: 0.5096\n",
            "Epoch 97/200\n",
            "782/782 - 48s - loss: 1.5689 - accuracy: 0.5382 - val_loss: 1.6363 - val_accuracy: 0.5115\n",
            "Epoch 98/200\n",
            "782/782 - 48s - loss: 1.5674 - accuracy: 0.5389 - val_loss: 1.6338 - val_accuracy: 0.5048\n",
            "Epoch 99/200\n",
            "782/782 - 48s - loss: 1.5665 - accuracy: 0.5391 - val_loss: 1.6256 - val_accuracy: 0.5176\n",
            "Epoch 100/200\n",
            "782/782 - 48s - loss: 1.5641 - accuracy: 0.5383 - val_loss: 1.6213 - val_accuracy: 0.5088\n",
            "Epoch 101/200\n",
            "782/782 - 48s - loss: 1.5628 - accuracy: 0.5414 - val_loss: 1.6368 - val_accuracy: 0.5064\n",
            "Epoch 102/200\n",
            "782/782 - 48s - loss: 1.5618 - accuracy: 0.5399 - val_loss: 1.6204 - val_accuracy: 0.5180\n",
            "Epoch 103/200\n",
            "782/782 - 48s - loss: 1.5611 - accuracy: 0.5401 - val_loss: 1.6154 - val_accuracy: 0.5222\n",
            "Epoch 104/200\n",
            "782/782 - 49s - loss: 1.5591 - accuracy: 0.5402 - val_loss: 1.6272 - val_accuracy: 0.5148\n",
            "Epoch 105/200\n",
            "782/782 - 50s - loss: 1.5583 - accuracy: 0.5405 - val_loss: 1.6277 - val_accuracy: 0.5150\n",
            "Epoch 106/200\n",
            "782/782 - 49s - loss: 1.5570 - accuracy: 0.5428 - val_loss: 1.6261 - val_accuracy: 0.5154\n",
            "Epoch 107/200\n",
            "782/782 - 50s - loss: 1.5556 - accuracy: 0.5429 - val_loss: 1.6194 - val_accuracy: 0.5154\n",
            "Epoch 108/200\n",
            "782/782 - 50s - loss: 1.5534 - accuracy: 0.5421 - val_loss: 1.6205 - val_accuracy: 0.5141\n",
            "Epoch 109/200\n",
            "782/782 - 50s - loss: 1.5529 - accuracy: 0.5429 - val_loss: 1.6146 - val_accuracy: 0.5178\n",
            "Epoch 110/200\n",
            "782/782 - 48s - loss: 1.5520 - accuracy: 0.5416 - val_loss: 1.6151 - val_accuracy: 0.5080\n",
            "Epoch 111/200\n",
            "782/782 - 49s - loss: 1.5505 - accuracy: 0.5442 - val_loss: 1.6119 - val_accuracy: 0.5186\n",
            "Epoch 112/200\n",
            "782/782 - 49s - loss: 1.5495 - accuracy: 0.5440 - val_loss: 1.6174 - val_accuracy: 0.5065\n",
            "Epoch 113/200\n",
            "782/782 - 50s - loss: 1.5489 - accuracy: 0.5435 - val_loss: 1.6105 - val_accuracy: 0.5164\n",
            "Epoch 114/200\n",
            "782/782 - 50s - loss: 1.5472 - accuracy: 0.5445 - val_loss: 1.6128 - val_accuracy: 0.5124\n",
            "Epoch 115/200\n",
            "782/782 - 49s - loss: 1.5458 - accuracy: 0.5446 - val_loss: 1.6103 - val_accuracy: 0.5195\n",
            "Epoch 116/200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeH9w4wXuxp2"
      },
      "source": [
        "#RMSProp\n",
        "tf.keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name=\"RMSprop\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_RMSProp = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpK8C_CAvhRq"
      },
      "source": [
        "#RMSProp + Nesterov\n",
        "tf.keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_RMSProp_Nesterov = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGUY6yHwwHHV"
      },
      "source": [
        "#AdaDelta\n",
        "tf.keras.optimizers.Adadelta(learning_rate=0.01, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_AdaDelta = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76RjjWYOwQg-"
      },
      "source": [
        "#Adam\n",
        "tf.keras.optimizers.Adam( learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name=\"Adam\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_Adam = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSJOkos7w8KG"
      },
      "source": [
        "#Plot\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(history_adagrad.history['loss'], label='AdaGrad')\n",
        "plt.plot(history_RMSProp.history['loss'], label='RMSProp')\n",
        "plt.plot(history_RMSProp_Nesterov.history['loss'], label='RMSProp+Nesterov')\n",
        "plt.plot(history_AdaDelta.history['loss'], label='AdaDelta')\n",
        "plt.plot(history_Adam.history['loss'], label='Adam')\n",
        "\n",
        "plt.title('Epoch vs training loss no dropout')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.legend(loc=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3LdGKg4yr_N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cHXBqO2y4Mm"
      },
      "source": [
        "3. Add dropout (probability 0.2 for input layer and 0.5 for hidden layers) and train the neural network again using all the five methods for 200 epochs. Compare the training loss with that in part 2. Which method performs the best ? For the five methods, compare their training time (to finish 200 epochs with dropout) to the training time in part 2 (to finish 200 epochs without dropout)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N5Ls5MCy-Vp"
      },
      "source": [
        "#model_dropout\n",
        "model_dropout = Sequential()\n",
        "model_dropout.add(Flatten())\n",
        "model_dropout.add(Dropout(0.2))\n",
        "model_dropout.add(Dense(1000, activation='relu', kernel_regularizer='l2', kernel_initializer='HeNormal'))\n",
        "model_dropout.add(Dropout(0.5))\n",
        "model_dropout.add(Dense(1000, activation='relu', kernel_regularizer='l2', kernel_initializer='HeNormal'))\n",
        "model_dropout.add(Dropout(0.5))\n",
        "model_dropout.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8fAxVUCzU8V"
      },
      "source": [
        "#Adagrad\n",
        "tf.keras.optimizers.Adagrad(learning_rate=0.01, initial_accumulator_value=0.1, epsilon=1e-07, name='Adagrad')\n",
        "model_dropout.compile(loss='categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_adagrad_dropout = model_dropout.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmT514HXzwl4"
      },
      "source": [
        "#RMSProp\n",
        "tf.keras.optimizers.RMSprop(learning_rate=0.01, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name=\"RMSprop\")\n",
        "model_dropout.compile(loss='categorical_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_RMSProp_dropout = model_dropout.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfEJ-QhIz_Ze"
      },
      "source": [
        "#RMSProp + Nesterov\n",
        "tf.keras.optimizers.Nadam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\")\n",
        "model_dropout.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_RMSProp_Nesterov_dropout = model_dropout.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp8Gt-rY0ECd"
      },
      "source": [
        "#AdaDelta\n",
        "tf.keras.optimizers.Adadelta(learning_rate=0.01, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
        "model_dropout.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_AdaDelta_dropout = model_dropout.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZjeMsoC0Wtv"
      },
      "source": [
        "#Adam\n",
        "tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name=\"Adam\")\n",
        "model_dropout.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "s_time = time.time()\n",
        "history_Adam_dropout = model_dropout.fit(X_train, y_train, batch_size=64, epochs=200, validation_data = (X_test, y_test), shuffle=True, verbose=2)\n",
        "e_time = time.time()\n",
        "print('Training Time:', e_time - s_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ETPqno0ckv"
      },
      "source": [
        "#Plot\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(history_adagrad_dropout.history['loss'], label='AdaGrad')\n",
        "plt.plot(history_RMSProp_dropout.history['loss'], label='RMSProp')\n",
        "plt.plot(history_RMSProp_Nesterov_dropout.history['loss'], label='RMSProp+Nesterov')\n",
        "plt.plot(history_AdaDelta_dropout.history['loss'], label='AdaDelta')\n",
        "plt.plot(history_Adam_dropout.history['loss'], label='Adam')\n",
        "\n",
        "plt.title('Epoch vs training loss no dropout')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.legend(loc=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkfsgvlsAiKc"
      },
      "source": [
        "4. Compare test accuracy of trained model for all the five methods from part 2 and part 3. Note that to\n",
        "calculate test accuracy of model trained using dropout you need to appropriately scale the weights (by\n",
        "the dropout probability). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUCUTRgaHzZD"
      },
      "source": [
        "## Solution\n",
        "\n",
        "Test accuracy for model without dropout: \n",
        "* Adagrad: \n",
        "* RMSProp: \n",
        "* RMSProp+N: \n",
        "* AdaDelta: \n",
        "* Adam: \n",
        "\n",
        "Test accuracy for model with dropout: \n",
        "* Adagrad:\n",
        "* RMSProp:\n",
        "* RMSProp+N: \n",
        "* AdaDelta: \n",
        "* Adam: \n",
        "\n",
        "* The test accuracy for non-dropout is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzldNWw-Ai08"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshWePplJYiA"
      },
      "source": [
        "## Problem 2 - Learning Rate, Batch Size, FashionMNIST\n",
        "\n",
        "Recall cyclical learning rate policy discussed in Lecture 4. The learning rate changes in cyclical manner between lrmin and lrmax, which are hyperparameters that need to be specified. For this problem you first need to read carefully the article referenced below as you will be making use of the code there (in Keras) and modifying it as needed. For those who want to work in Pytorch there are open source implementations of this policy available which you can easily search for and build over them. You will work with FashionM-NIST dataset and MiniGoogLeNet (described in reference). If you cannot get MiniGoogleNet code from the reference you can do this question using LeNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV29BHRqJv4E"
      },
      "source": [
        "1. Summarize FashionMNIST dataset, total dataset size, training set size, validation set size, number of\n",
        "classes, number of images per class. Show any 3 representative images from any 3 classes in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GC_TY8GJsy0"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "print(np.unique(y_train,return_counts=True))\n",
        "print(np.unique(y_test,return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlitsIqJKxw9"
      },
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(X_train[0])\n",
        "plt.colorbar()\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(X_train[10])\n",
        "plt.colorbar()\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(X_train[20])\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ew_ZT4lLaFM"
      },
      "source": [
        "2. Fix batch size to 64 and start with 10 candidate learning rates between $10^{-9}$ and $10^1$ and train your\n",
        "model for 5 epochs. Plot the training loss as a function of learning rate. You should see a curve like\n",
        "Figure 3 in reference below. From that figure identify the values of $lr_{min}$ and $lr_{max}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9tSAQuULD2-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}